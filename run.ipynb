{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils import PianoDataset\n",
    "from model import LSTMModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('processed_data.pkl', 'rb') as f:\n",
    "    (normalized_sequences, encoded_labels, vocab, d_min, d_max) = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "dataset = PianoDataset(normalized_sequences, encoded_labels)\n",
    "\n",
    "batch_size = 32\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# create training/val subsets, DataLoader objects\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x_notes, x_durations, y_notes, y_durations in train_loader:\n",
    "    print(\"X Notes shape:\", x_notes.shape)       # Should be (batch_size, 32)\n",
    "    print(\"X Durations shape:\", x_durations.shape)  # Should be (batch_size, 32)\n",
    "    print(\"Y Notes shape:\", y_notes.shape)     # Should be (batch_size,)\n",
    "    print(\"Y Durations shape:\", y_durations.shape)     # Should be (batch_size,)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(test_data_x, test_data_y, model):\n",
    "\t#tensor_test_x = torch.IntTensor(test_data_x)\n",
    "\t#tensor_test_y = torch.LongTensor(test_data_y)\n",
    "\ttest_outputs = model(test_data_x)\t\n",
    "\ttest_labels = test_data_y\n",
    "\t\n",
    "\tpredict_label = torch.argmax(test_outputs, dim=1)\t\n",
    "\t\n",
    "\tacc = (predict_label == test_labels).sum() / float(len(test_labels))\n",
    "\treturn float(acc)\n",
    "\n",
    "def note_accuracy(outputs, labels):\n",
    "\tpredict_label = torch.argmax(outputs, dim=1)\t\n",
    "\t\n",
    "\tacc = (predict_label == labels).sum() / float(len(labels))\n",
    "\treturn float(acc)\n",
    "\n",
    "def duration_accuracy(outputs, labels):\n",
    "\tpass\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_notes = 0\n",
    "    correct_notes = 0\n",
    "    total_duration_error = 0\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "        for x_notes, x_durations, y_notes, y_durations in test_loader:\n",
    "            # Move data to the appropriate device (GPU or CPU)\n",
    "            #x_notes, x_durations = x_notes.to(device), x_durations.to(device)\n",
    "            #y_notes, y_durations = y_notes.to(device), y_durations.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            note_pred, duration_pred = model(x_notes, x_durations)\n",
    "\n",
    "            # Notes: Compare predicted class with ground truth\n",
    "            predicted_notes = torch.argmax(note_pred, dim=1)  # Get index of highest logit\n",
    "            correct_notes += (predicted_notes == y_notes).sum().item()\n",
    "            total_notes += y_notes.size(0)\n",
    "\n",
    "            # Durations: Compute regression error\n",
    "            total_duration_error += F.l1_loss(duration_pred.squeeze(), y_durations, reduction='sum').item()\n",
    "\n",
    "            num_samples += y_durations.size(0)\n",
    "\n",
    "    # Calculate metrics\n",
    "    note_accuracy = correct_notes / total_notes\n",
    "    avg_duration_error = total_duration_error / num_samples\n",
    "\n",
    "    return note_accuracy, avg_duration_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "vocab_size = 1840\n",
    "embedding_dim = 64\n",
    "hidden_dim1 = 256\n",
    "hidden_dim2 = 128\n",
    "model = LSTMModel(vocab_size, embedding_dim, hidden_dim1, hidden_dim2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_state_dict(torch.load(\"model_parameters.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_acc, dur_err = evaluate(model, test_loader)\n",
    "print(f'{note_acc:.4f}')\n",
    "print(f'{dur_err:.4f}')\n",
    "results = {0: [None, note_acc, dur_err]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Params\n",
    "num_epochs = 1\n",
    "alpha = 0.1\n",
    "criterion_note = nn.CrossEntropyLoss()\n",
    "criterion_duration = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for x_notes, x_durations, y_note, y_duration in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        note_pred, duration_pred = model(x_notes, x_durations)\n",
    "        \n",
    "        # Compute combined loss\n",
    "        loss_note = criterion_note(note_pred, y_note)\n",
    "        loss_duration = criterion_duration(duration_pred.squeeze(), y_duration)\n",
    "        loss = loss_note + alpha*loss_duration \n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    note_acc, dur_err = evaluate(model, test_loader)\n",
    "    elapsed = time.time() - start\n",
    "    t = str(timedelta(seconds=elapsed))\n",
    "    print(f\"Epoch {epoch + 1}: Time - {t}, Loss - {running_loss:.6f}, Note Acc - {note_acc:.4f}, Dur Err - {dur_err:.4f}\")\n",
    "    results[epoch+1] = [running_loss, note_acc, dur_err]\n",
    "    torch.save(model.state_dict(), f'models/{epoch+1}_parameters.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LoFi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
