# LoFiNet
A deep learning model for generating LoFi hip hop beats by predicting the next note (both note value and duration) in a sequence based on previous notes. 

## Model 
The model architecture uses an LSTM to capture temporal dependencies in the melodic sequences, then employs two parallel fully connected layers for two distinct prediction tasks: determining the next note (categorical classification) and its duration (regression). An implementation of the model in PyTorch appears in `model.py` and a notebook running the model is provided in `run.ipynb`. Functions to generate music with the trained model are provided in the notebook `generate.ipynb`.

The model expects as input a sequence of 32 notes, and is used to predict the 33rd. In symbolic representation,the note at index *i* is played at the *i*th quarter-note after the start. Here, each note is a tuple (value, duration). The value of the note takes one of three categories: an individual note, a chord, or a rest. An individual note is one pitch among {A, B, C, D, E, F, G, A#, C#, D#, F#, G#} at a particular octave (0-10), converted into its MIDI code (an integer in the range (0,127), partially extending into additional octaves). For example, 60 for represents C in the 4th octave. A chord is simply 2-4 notes played at the same time (a set of MIDI codes), and a rest indicates the absence of a note being played at time *i*. The duration corresponds to how long a note/chord should be played for with respect to the quarter-note time unit (it does not have to be in units of quarter notes, but ensures consistency across different tempos). Rest notes always last exactly one quarter-note.

## Data and preprocessing
To train the model, I used LoFi piano samples and jazz piano melodies in the form of MIDI (Musical Instrument Digital Interface) files, a compact format for music data. A lot of preprocessing is necessary before the raw data from the MIDI files can be passed into the model. In particular, the vast number of possible notes and chords leads to a huge vocabulary. The preprocessing pipeline includes matching heuristics to replace low-frequency notes/chords with high frequency ones, reducing the vocabulary size. Additionally, songs must be partitioned into 32-length sequences. The entire preprocessing pipeline is available in the notebook `preprocess.ipynb`. Specifically, run the cell that calls the `gather_all(dset)` function, replacing dset with the name of the directory containing the MIDI files, and also run each cell that follows. This will save the processed sequences and other information in a pickle file which can be used later. Additionally, the music21 package is sometimes unable to isolate different instruments in a MIDI file. If extracting the piano sequence from a MIDI file proves difficult, try first using the `extract_midi.py` script which uses a different MIDI processing package to separate the instruments. 
